{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import composer\n",
    "from composer.datasets import coco_mmdet\n",
    "from composer.models import composer_yolox\n",
    "from torch.utils.data import DataLoader\n",
    "from composer.datasets.coco_mmdet import mmdet_collate, mmdet_get_num_samples\n",
    "from composer.core.data_spec import DataSpec\n",
    "from composer.loggers import InMemoryLogger, LogLevel, WandBLogger\n",
    "\n",
    "\n",
    "\n",
    "import logging, sys # disable logging in notebook\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "torch.manual_seed(42) # For replicability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = coco_mmdet(path='../../data/coco', split='train')\n",
    "val_dataset = coco_mmdet(path='../../data/coco', split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = composer_yolox(model_name='yolox-s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=mmdet_collate, shuffle=True, drop_last=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=mmdet_collate, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.persistent_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = composer.optim.DecoupledSGDW(\n",
    "    model.parameters(), # Model parameters to update\n",
    "    lr=0.01, # Peak learning rate\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4,\n",
    "    nesterov=True # If this looks large, it's because its not scaled by the LR as in non-decoupled weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = composer.optim.CosineAnnealingWithWarmupScheduler(\n",
    "    t_warmup=\"30ep\", # Warm up over 30 epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epochs = \"300ep\" # Train for 3 epochs because we're assuming Colab environment and hardware\n",
    "\n",
    "trainer = composer.trainer.Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=DataSpec(train_loader, get_num_samples_in_batch=mmdet_get_num_samples),\n",
    "    eval_dataloader=DataSpec(val_loader, get_num_samples_in_batch=mmdet_get_num_samples),\n",
    "    max_duration=train_epochs,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=lr_scheduler,\n",
    "    # train_subset_num_batches=10,\n",
    "    device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    precision='fp32' # currently, simOTA matcher will not run with AMP\n",
    "    grad_accum=1,\n",
    "    loggers=[InMemoryLogger(log_level=LogLevel.BATCH), WandBLogger(project='yolox-test')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
